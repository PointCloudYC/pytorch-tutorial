# explore

## tldr
**all deep learning models share similar pattern, consisting of 4 core components, including dataset, model, training, and evaluating**
**DL paradigm i.e. find optimal model parameters by numerical iteration(i.e. gradient descent); the key here is how to compute the gradient of parameters(weights) efficiently? use backprop which is implemented in pytorch/tf using autograd/audodiff.**

- (X,Y); dataset and dataloader
- computation graph; Loss = f(W|X,Y)
  - model; define how to get pred using X and operations(layers/blocks)
  - loss;
- learning; update weights using gradient descent(optimizer)
  - define optimizer(i.e. update rules) and metrics(i.e. measure distance between pred and ground truth);
  - training; compute gradients and update weights;
  - evaluating; compute metrics using learned weights;
- visualization;

## overview
- 01-baiscs; basic concepts(tensor, autograd, GD), basic models(LR,linear_reg,DNN)
- 02-intermediate; CV models, including CNNs(ALexNet, ResNet), RNNs, BiRNNs, lang_models
- 03-advanced; GANs, img-captioning, neural style transfer, and VAE
- 04-utils; how to use tensorboard

## review code
- 01-basics;
  - tensor for **array programming with gpu support**
    - create tensor(value,type,shape,etc) similar to numpy/tensorflow;but 2 special attributes `requires_grad`(default is true) and `device`(default is 'cpu')
    - tensor operation i.e. building computation graph; e.g. y=w*x+b, y will has an attribute called `grad_fn`
    - autograd; use `backward()` to compute gradient for each element in the graph; can access by `x.grad` (tensor type)
  - layers(a type of built-in operations)
    - Linear; fc layers, u can access weight\bias by `linear.weight\bias`
    - Conv2D; conv2d layers
  - optmizer(how to update weigh/bias), loss(loss/objective function)
    - create; `optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)` and `criterion = nn.MSELoss()`
    - trigger; `loss=criterion(pred,y)` then `loss.backward` and then update weights/biases(layer/model.parameters) `optimizer.step()`
  - example 1/2/3: linear regression, logistic_regression, DNN(feedforward NNs);



- 02-intermediate; DL coding pattern is similar; difference are on dataset part and models. 
  - CNNs; AlexNet;
  - ResNet; compared w. previous models(feedforward networks, logistic regression, etc.), **the pattern is similar except model(i.e. computation graph defining how tensor input is forwarded to map input to good features then predict scores) is different**.
  - RNNs; LSTM, need sequential data (i.e. data with time features, [B,sequences,features])
  - RNNs; BiRNNs

- 03-advanced;
  - VAE;
  - GAN;

- 04-utils;
  - tensorboard visualization(1.sclar, 2.images, 3.histogram of weights and grads)

## todos

- [x] replicate AlexNet
- [x] replicate ResNet
- ~~[ ] replicate GAN~~
- ~~[ ] replicate VAE~~
- ~~[ ] replicate style transfer~~
- ~~[ ] replicate image caption~~
